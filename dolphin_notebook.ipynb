{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#Source Files\n",
    "from lib.partition import split_by_day\n",
    "import lib.file_utilities as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Files\n",
    "\n",
    "Given a filepath to the recording directories:\n",
    "- retrieve x amount of files for each species\n",
    "- for each file get its metadata\n",
    "- split the files for each species into dictionaries keyed by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory of each species click folder\n",
    "ggdir = os.path.abspath(\"./features/Gg\")\n",
    "lodir = os.path.abspath(\"./features/Lo/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()   # enable interactive plotting\n",
    "\n",
    "#use_onlyN = np.Inf  # use this to get all files\n",
    "use_onlyN = 10\n",
    "#get list of click files for each species\n",
    "ggfiles = util.get_files(ggdir, \".czcc\", use_onlyN)\n",
    "lofiles = util.get_files(lodir, \".czcc\", use_onlyN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting information about files and loading features for  10 recordings.\n",
      "Reading file 0/10\n",
      "Extracting information about files and loading features for  10 recordings.\n",
      "Reading file 0/10\n"
     ]
    }
   ],
   "source": [
    "#create lists of tuples (.site, .label, .start, .features) for each species\n",
    "ggmeta_data = util.parse_files(ggfiles)\n",
    "lometa_data = util.parse_files(lofiles)\n",
    "test_meta_data = ggmeta_data[0]\n",
    "\n",
    "#create dictionaries keyed by day\n",
    "#key=datetime.start value=list[tuples (.site, label, .start, .features)]\n",
    "gg_day_dict = split_by_day(ggmeta_data)\n",
    "lo_day_dict = split_by_day(lometa_data)\n",
    "\n",
    "#create lists of days in dictionaries\n",
    "gg_keys = list(gg_day_dict.keys())\n",
    "lo_keys = list(lo_day_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Days\n",
    "\n",
    "\n",
    "Using the dictionaries created:\n",
    "- split the keyed days into training and testing days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of lists for training days and test days\n",
    "#<species>_train_test_days[0] is train\n",
    "#<species>_train_test_days[1] is test\n",
    "gg_train_test_days = train_test_split(gg_keys, test_size=0.33, random_state=42)\n",
    "lo_train_test_days = train_test_split(lo_keys, test_size=0.33, random_state=42)\n",
    "\n",
    "gg_train_days = gg_train_test_days[0]\n",
    "gg_test_days = gg_train_test_days[1]\n",
    "\n",
    "lo_train_days = lo_train_test_days[0]\n",
    "lo_test_days = lo_train_test_days[1]\n",
    "\n",
    "#print(\"GG Train Days: {}\\n {} \\n\".format(len(gg_train_days), gg_train_days))\n",
    "#print(\"GG Test Days: {}\\n {} \\n\".format(len(gg_test_days), gg_test_days))\n",
    "\n",
    "#print(\"LO Train Days: {}\\n {} \\n\".format(len(lo_train_days), lo_train_days))\n",
    "#print(\"LO Test Days: {}\\n {} \\n\".format(len(lo_test_days), lo_test_days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Data\n",
    "\n",
    "Given a list of training days for each species:\n",
    "- create a large tensor of examples of shape (x, 20)\n",
    "- create a large vector of labels of size (x, 2)\n",
    "- labels are one hot encoded for each species\n",
    "- Gg: [1,0]\n",
    "- Lo: [0,1]\n",
    "\n",
    "- Class Weights\n",
    "    - uses compute_class_weights from sklearn library to compute weights to alleviate class imbalance\n",
    "    - the compute class_imbalance library does not work with one hot encoded labels so a second label array\n",
    "    one_d_labels has been created that is 1 dimensional\n",
    "    - this array is used to create the weight dictionary that will be passed to class_weight parameter in fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(train_days_gg, train_days_lo):\n",
    "    train_tensor_examples = []\n",
    "    train_tensor_labels = []\n",
    "    one_d_labels = []\n",
    "    \n",
    "    #iterate through gg training days\n",
    "    for day in train_days_gg: \n",
    "        #print(\"Day: {}\\n\".format(day))\n",
    "        #iterate through recordings in day\n",
    "        for recording in gg_day_dict[day]:\n",
    "            #get row, col to make correct amount of labels\n",
    "            row, col = recording.features.shape\n",
    "            #print(\"This recording has rows: {} cols: {} \\n\".format(row, col))\n",
    "            train_tensor_examples.append(recording.features)\n",
    "            #create row amount of gg labels\n",
    "            train_tensor_labels.append([[1,0]] * row)\n",
    "            one_d_labels.append(np.full(row, 0))\n",
    "            \n",
    "    \n",
    "    #iterate through lo training days\n",
    "    for day in train_days_lo:\n",
    "        #print(\"Day: {}\\n\".format(day))\n",
    "        #iterate through recordings in day\n",
    "        for recording in lo_day_dict[day]:\n",
    "            #get row, col to make correct amount of labels\n",
    "            row, col = recording.features.shape\n",
    "            #print(\"This recording has rows: {} cols: {}\\n\".format(row, col))\n",
    "            train_tensor_examples.append(recording.features)\n",
    "            #create row amount of lo labels\n",
    "            train_tensor_labels.append([[0,1]] * row)\n",
    "            one_d_labels.append(np.full(row, 1))\n",
    "            \n",
    "           \n",
    "    #stack all recordings on top of one another, creates (total # training examples, 20) ndarray\n",
    "    big_train_examples = np.concatenate(train_tensor_examples, axis=0)\n",
    "    #combines all labels together, create vector of size (total # training examples)\n",
    "    one_d_labels = np.concatenate(one_d_labels, axis=0)\n",
    "    big_train_labels = np.concatenate(train_tensor_labels, axis=0)\n",
    "    \n",
    "    #print(\"Shape of training data: {}\\n\".format(big_train_examples.shape))\n",
    "    #print(\"Size of label vector: {}\\n\".format(big_train_labels.size))\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(one_d_labels), y=one_d_labels)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    return big_train_examples, big_train_labels, class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24792, 20)\n",
      "(24792, 2)\n",
      "{0: 0.8976754290679991, 1: 1.1286533733952473}\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, class_weights = create_train_data(gg_train_days, lo_train_days)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Testing Data\n",
    "\n",
    "Given a list of test days for each species:\n",
    "- Examples\n",
    "    - create a list of batches of features\n",
    "    - each batch is of shape (100, 20)\n",
    "    - each click in a batch comes from the same file\n",
    "\n",
    "- Labels\n",
    "    - create a list of batches of labels\n",
    "    - each batch is of shape (100, 2)\n",
    "    - each label in a batch comes from the same file\n",
    "    - labels are still one hot encoded, refer to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to making training data, but we don't combine everything\n",
    "#instead we create a list that we can iterate through and test on each element separately\n",
    "def create_test_data(test_days_gg, test_days_lo):\n",
    "    test_tensor_examples = []\n",
    "    test_tensor_labels = []\n",
    "    \n",
    "    #iterate through gg test days\n",
    "    for day in test_days_gg:\n",
    "        for recording in gg_day_dict[day]:\n",
    "            if len(recording.features) < 100:\n",
    "                continue\n",
    "            groups, groups_labels = hunnit_group(recording)\n",
    "            test_tensor_examples.append(groups)\n",
    "            test_tensor_labels.append(groups_labels)\n",
    "            \n",
    "    for day in test_days_lo:\n",
    "        for recording in lo_day_dict[day]:\n",
    "            if len(recording.features) < 100:\n",
    "                continue\n",
    "            groups, groups_labels = hunnit_group(recording)\n",
    "            test_tensor_examples.append(groups)\n",
    "            test_tensor_labels.append(groups_labels)\n",
    "            \n",
    "    big_test_examples = np.concatenate(test_tensor_examples, axis=0)\n",
    "    big_test_labels = np.concatenate(test_tensor_labels, axis=0)\n",
    "            \n",
    "    return big_test_examples, big_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create batches, takes a metadata tuple\n",
    "def hunnit_group(recording):\n",
    "    recording_tensor = recording.features\n",
    "    print(\"Number of rows in features {}\".format(len(recording_tensor)))\n",
    "    if recording.label == \"Gg\":\n",
    "        print(\"Label is Gg: [1,0]\\n\")\n",
    "        label = 0\n",
    "    else:\n",
    "        print(\"Label is Lo: [0,1]\\n\")\n",
    "        label = 1\n",
    "    # list comprehension to grab as many 100 clicks as we can\n",
    "    hunnit_batches = [recording_tensor[x:x+100] for x in range(0, len(recording_tensor), 100) if ((len(recording_tensor) - x) >= 100)]\n",
    "    # for each 100 clicks batch create a corresponding label vector\n",
    "    label_batches = [np.array(label)] * len(hunnit_batches)\n",
    "    return hunnit_batches, label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in features 120\n",
      "Label is Gg: [1,0]\n",
      "\n",
      "Number of rows in features 360\n",
      "Label is Gg: [1,0]\n",
      "\n",
      "Number of rows in features 1180\n",
      "Label is Gg: [1,0]\n",
      "\n",
      "Number of rows in features 4770\n",
      "Label is Gg: [1,0]\n",
      "\n",
      "Number of rows in features 108\n",
      "Label is Lo: [0,1]\n",
      "\n",
      "Number of rows in features 5158\n",
      "Label is Lo: [0,1]\n",
      "\n",
      "Number of rows in features 5294\n",
      "Label is Lo: [0,1]\n",
      "\n",
      "Number of rows in features 10098\n",
      "Label is Lo: [0,1]\n",
      "\n",
      "Number of batches to go through: 266\n",
      "\n",
      "(100, 20)\n",
      "100\n",
      "[[ 2.55555313e+02 -2.51788692e+01 -3.47140808e+01 ... -2.55522311e-01\n",
      "   5.52612782e-01  1.32973158e+00]\n",
      " [ 5.70930290e+01  1.44229145e+01 -2.53474312e+01 ...  1.12609797e+01\n",
      "   7.46982479e+00  6.09665573e-01]\n",
      " [ 7.37710114e+01  8.85740471e+00 -5.24337540e+01 ...  1.45737171e+01\n",
      "   4.26936102e+00 -2.67292809e+00]\n",
      " ...\n",
      " [ 6.83872833e+01  1.31987944e+01 -2.66798344e+01 ...  1.09381065e+01\n",
      "   5.23020029e+00 -6.38920498e+00]\n",
      " [ 7.55746841e+01  2.49358845e+01 -2.42202511e+01 ...  1.67088585e+01\n",
      "   4.40210676e+00 -3.58579278e+00]\n",
      " [ 2.90775024e+02 -6.53981247e+01 -6.12543182e+01 ... -5.63372135e-01\n",
      "  -1.82620287e-01  9.26660240e-01]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = create_test_data(gg_test_days, lo_test_days)\n",
    "\n",
    "print(\"Number of batches to go through: {}\\n\".format(len(X_test)))\n",
    "print(X_test[265].shape)\n",
    "print(len(X_test[265]))\n",
    "#print(len(Y_test[265]))\n",
    "print(X_test[265])\n",
    "#print(Y_test[265])\n",
    "print(Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=20, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Erick\\anaconda3\\envs\\DolphinIdentificationNew\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/5\n",
      "1550/1550 [==============================] - 12s 682us/step - loss: 1.5174 - accuracy: 0.7927\n",
      "Epoch 2/5\n",
      "1550/1550 [==============================] - 1s 670us/step - loss: 0.5551 - accuracy: 0.8203\n",
      "Epoch 3/5\n",
      "1550/1550 [==============================] - 1s 680us/step - loss: 0.4481 - accuracy: 0.8286\n",
      "Epoch 4/5\n",
      "1550/1550 [==============================] - 1s 720us/step - loss: 0.4040 - accuracy: 0.8415\n",
      "Epoch 5/5\n",
      "1550/1550 [==============================] - 1s 677us/step - loss: 0.3791 - accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "nn_model = model.fit(X_train, Y_train, epochs=5, batch_size=16, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = []\n",
    "for x in X_test:\n",
    "    out = model.predict(x)\n",
    "    sum_prob = np.sum(np.log(out), axis = 0)\n",
    "    #print(sum_prob)\n",
    "    #print(np.argmax(sum_prob))\n",
    "    pred_classes.append(np.argmax(sum_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "confoos = confusion_matrix(Y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 62,   0],\n",
       "       [  3, 201]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confoos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
