{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9fe1c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#Source Files\n",
    "from lib.partition import split_by_day\n",
    "import lib.file_utilities as util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846665a",
   "metadata": {},
   "source": [
    "# Parsing Files\n",
    "\n",
    "Given a filepath to the recording directories:\n",
    "- retrieve x amount of files for each species\n",
    "- for each file get its metadata\n",
    "- split the files for each species into dictionaries keyed by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9d14ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory of each species click folder\n",
    "ggdir = os.path.abspath(\"./features/Gg\")\n",
    "lodir = os.path.abspath(\"./features/Lo/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c83e76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()   # enable interactive plotting\n",
    "\n",
    "#use_onlyN = np.Inf  # use this to get all files\n",
    "use_onlyN = 10\n",
    "#get list of click files for each species\n",
    "ggfiles = util.get_files(ggdir, \".czcc\", use_onlyN)\n",
    "lofiles = util.get_files(lodir, \".czcc\", use_onlyN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "075bfe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting information about files and loading features for  10 recordings.\n",
      "Reading file 0/10\n",
      "Extracting information about files and loading features for  10 recordings.\n",
      "Reading file 0/10\n"
     ]
    }
   ],
   "source": [
    "#create lists of tuples (.site, .label, .start, .features) for each species\n",
    "ggmeta_data = util.parse_files(ggfiles)\n",
    "lometa_data = util.parse_files(lofiles)\n",
    "test_meta_data = ggmeta_data[0]\n",
    "\n",
    "#create dictionaries keyed by day\n",
    "#key=datetime.start value=list[tuples (.site, label, .start, .features)]\n",
    "gg_day_dict = split_by_day(ggmeta_data)\n",
    "lo_day_dict = split_by_day(lometa_data)\n",
    "\n",
    "#create lists of days in dictionaries\n",
    "gg_keys = list(gg_day_dict.keys())\n",
    "lo_keys = list(lo_day_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65971c4d",
   "metadata": {},
   "source": [
    "# Splitting Days\n",
    "\n",
    "\n",
    "Using the dictionaries created:\n",
    "- split the keyed days into training and testing days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72a4e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of lists for training days and test days\n",
    "#<species>_train_test_days[0] is train\n",
    "#<species>_train_test_days[1] is test\n",
    "gg_train_test_days = train_test_split(gg_keys, test_size=0.33, random_state=42)\n",
    "lo_train_test_days = train_test_split(lo_keys, test_size=0.33, random_state=42)\n",
    "\n",
    "gg_train_days = gg_train_test_days[0]\n",
    "gg_test_days = gg_train_test_days[1]\n",
    "\n",
    "lo_train_days = lo_train_test_days[0]\n",
    "lo_test_days = lo_train_test_days[1]\n",
    "\n",
    "#print(\"GG Train Days: {}\\n {} \\n\".format(len(gg_train_days), gg_train_days))\n",
    "#print(\"GG Test Days: {}\\n {} \\n\".format(len(gg_test_days), gg_test_days))\n",
    "\n",
    "#print(\"LO Train Days: {}\\n {} \\n\".format(len(lo_train_days), lo_train_days))\n",
    "#print(\"LO Test Days: {}\\n {} \\n\".format(len(lo_test_days), lo_test_days))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694cd5f",
   "metadata": {},
   "source": [
    "# Create Training Data\n",
    "\n",
    "Given a list of training days for each species:\n",
    "- create a large tensor of examples of shape (x, 20)\n",
    "- create a large vector of labels of size (x, 2)\n",
    "- labels are one hot encoded for each species\n",
    "- Gg: [1,0]\n",
    "- Lo: [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1312b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(train_days_gg, train_days_lo):\n",
    "    train_tensor_examples = []\n",
    "    train_tensor_labels = []\n",
    "    \n",
    "    #iterate through gg training days\n",
    "    for day in train_days_gg: \n",
    "        #print(\"Day: {}\\n\".format(day))\n",
    "        #iterate through recordings in day\n",
    "        for recording in gg_day_dict[day]:\n",
    "            #get row, col to make correct amount of labels\n",
    "            row, col = recording.features.shape\n",
    "            #print(\"This recording has rows: {} cols: {} \\n\".format(row, col))\n",
    "            train_tensor_examples.append(recording.features)\n",
    "            #create row amount of gg labels\n",
    "            train_tensor_labels.append([[1,0]] * row)\n",
    "            \n",
    "    \n",
    "    #iterate through lo training days\n",
    "    for day in train_days_lo:\n",
    "        #print(\"Day: {}\\n\".format(day))\n",
    "        #iterate through recordings in day\n",
    "        for recording in lo_day_dict[day]:\n",
    "            #get row, col to make correct amount of labels\n",
    "            row, col = recording.features.shape\n",
    "            #print(\"This recording has rows: {} cols: {}\\n\".format(row, col))\n",
    "            train_tensor_examples.append(recording.features)\n",
    "            #create row amount of lo labels\n",
    "            train_tensor_labels.append([[0,1]] * row)\n",
    "           \n",
    "    #stack all recordings on top of one another, creates (total # training examples, 20) ndarray\n",
    "    big_train_examples = np.concatenate(train_tensor_examples, axis=0)\n",
    "    #combines all labels together, create vector of size (total # training examples)\n",
    "    big_train_labels = np.concatenate(train_tensor_labels, axis=0)\n",
    "    \n",
    "    #print(\"Shape of training data: {}\\n\".format(big_train_examples.shape))\n",
    "    #print(\"Size of label vector: {}\\n\".format(big_train_labels.size))    \n",
    "    \n",
    "    return big_train_examples, big_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ddbee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24792, 20)\n",
      "(24792, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = create_train_data(gg_train_days, lo_train_days)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3d8b4",
   "metadata": {},
   "source": [
    "# Create Testing Data\n",
    "\n",
    "Given a list of test days for each species:\n",
    "- Examples\n",
    "    - create a list of batches of features\n",
    "    - each batch is of shape (100, 20)\n",
    "    - each click in a batch comes from the same file\n",
    "\n",
    "- Labels\n",
    "    - create a list of batches of labels\n",
    "    - each batch is of shape (100, 2)\n",
    "    - each label in a batch comes from the same file\n",
    "    - labels are still one hot encoded, refer to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aedc440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to making training data, but we don't combine everything\n",
    "#instead we create a list that we can iterate through and test on each element separately\n",
    "def create_test_data(test_days_gg, test_days_lo):\n",
    "    test_tensor_examples = []\n",
    "    test_tensor_labels = []\n",
    "    \n",
    "    #iterate through gg test days\n",
    "    for day in test_days_gg:\n",
    "        for recording in gg_day_dict[day]:\n",
    "            if len(recording.features) < 100:\n",
    "                continue\n",
    "            groups, groups_labels = hunnit_group(recording)\n",
    "            test_tensor_examples.append(groups)\n",
    "            test_tensor_labels.append(groups_labels)\n",
    "            \n",
    "    for day in test_days_lo:\n",
    "        for recording in lo_day_dict[day]:\n",
    "            if len(recording.features) < 100:\n",
    "                continue\n",
    "            groups, groups_labels = hunnit_group(recording)\n",
    "            test_tensor_examples.append(groups)\n",
    "            test_tensor_labels.append(groups_labels)\n",
    "            \n",
    "    big_test_examples = np.concatenate(test_tensor_examples, axis=0)\n",
    "    big_test_labels = np.concatenate(test_tensor_labels, axis=0)\n",
    "            \n",
    "    return big_test_examples, big_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c1056f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create batches, takes a metadata tuple\n",
    "def hunnit_group(recording):\n",
    "    recording_tensor = recording.features\n",
    "    print(\"Number of rows in features {}\".format(len(recording_tensor)))\n",
    "    if recording.label == \"Gg\":\n",
    "        print(\"Label is Gg: [1,0]\\n\")\n",
    "        label = [1,0]\n",
    "    else:\n",
    "        print(\"Label is Lo: [0,1]\\n\")\n",
    "        label = [0,1]\n",
    "    # list comprehension to grab as many 100 clicks as we can\n",
    "    hunnit_batches = [recording_tensor[x:x+100] for x in range(0, len(recording_tensor), 100) if ((len(recording_tensor) - x) >= 100)]\n",
    "    # for each 100 clicks batch create a corresponding label vector\n",
    "    label_batches = [np.array([label] * 100)] * len(hunnit_batches)\n",
    "    return hunnit_batches, label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a6727f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in features 120\n",
      "Label is Gg: [1,0]\n",
      "\n",
      "Number of rows in features 360\n",
      "Label is Gg: [1,0]\n",
      "\n",
      "Number of rows in features 1180\n",
      "Label is Gg: [1,0]\n",
      "\n",
      "Number of rows in features 4770\n",
      "Label is Gg: [1,0]\n",
      "\n",
      "Number of rows in features 108\n",
      "Label is Lo: [0,1]\n",
      "\n",
      "Number of rows in features 5158\n",
      "Label is Lo: [0,1]\n",
      "\n",
      "Number of rows in features 5294\n",
      "Label is Lo: [0,1]\n",
      "\n",
      "Number of rows in features 10098\n",
      "Label is Lo: [0,1]\n",
      "\n",
      "Number of batches to go through: 266\n",
      "\n",
      "(100, 20)\n",
      "100\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = create_test_data(gg_test_days, lo_test_days)\n",
    "\n",
    "#print(len(X_test[7]))\n",
    "print(\"Number of batches to go through: {}\\n\".format(len(X_test)))\n",
    "print(X_test[8].shape)\n",
    "print(len(Y_test[7]))\n",
    "print(Y_test[7].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba677cea",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d43bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b350679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=20, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038db43c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "359634d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11595/11595 [==============================] - 8s 623us/step - loss: 0.7105 - accuracy: 0.8334\n",
      "Epoch 2/5\n",
      "11595/11595 [==============================] - 7s 621us/step - loss: 0.3883 - accuracy: 0.8476\n",
      "Epoch 3/5\n",
      "11595/11595 [==============================] - 7s 614us/step - loss: 0.3800 - accuracy: 0.8507\n",
      "Epoch 4/5\n",
      "11595/11595 [==============================] - 8s 652us/step - loss: 0.3766 - accuracy: 0.8515\n",
      "Epoch 5/5\n",
      "11595/11595 [==============================] - 7s 635us/step - loss: 0.3738 - accuracy: 0.8528\n"
     ]
    }
   ],
   "source": [
    "nn_model = model.fit(X_train, Y_train, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227929c",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d1a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
